%
% File naacl2019.tex
%
%% Based on the style files for ACL 2018 and NAACL 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{naaclhlt2019}
\usepackage{times}
\usepackage{latexsym}
\usepackage[utf8]{inputenc}

\usepackage{multirow}
\usepackage{xspace}
\usepackage{url}
\usepackage{tabularx}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{paralist}
\usepackage{subfig}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xstring}
\usepackage{scalefnt}
\usepackage{bm}
\usepackage{paralist}
%\usepackage{microtype} % microtype breaks non-ascii use in bib
\usepackage{tablefootnote}
\usepackage{subfig}
\usepackage{capt-of}
\usepackage{latexsym}
\usepackage{paralist}
\usepackage{calc}
\usepackage{booktabs}
\usepackage{url}
\usepackage{scrextend}

\definecolor{lightgreen}{RGB}{200,255,200}
\definecolor{lightblue}{RGB}{200,200,255}
\definecolor{lightred}{RGB}{255,200,200}

\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\mt}{\textsc{Mt}\xspace}
\newcommand{\blse}{\textsc{Blse}\xspace}
\newcommand{\barista}{\textsc{Barista}\xspace}
\newcommand{\vecmap}{\textsc{VecMap}\xspace}
\newcommand{\muse}{\textsc{Muse}\xspace}

\newcommand{\rt}[1]{\rotatebox{90}{#1}}
\newcommand{\rrt}[1]{\rotatebox{45}{#1}}

\newcommand{\ie}{\textit{i.\,e.}\xspace}
\newcommand{\eg}{\textit{e.\,g.}\xspace}
\newcommand{\F}{$\text{F}_1$\xspace}


%jer: automatically turned off when aclfinalcopy is turned on
\usepackage{annotates}
\newcommand{\todo}[1]{\comment{Todo}{#1}}
\newcommand{\jer}[1]{\comment{Jeremy}{#1}}
\newcommand{\toni}[1]{\comment{Toni}{#1}}
\newcommand{\alex}[1]{\comment{Alex}{#1}}

\newcommand{\jeranswer}[1]{\answer{Jeremy}{#1}}
\newcommand{\alexanswer}[1]{\answer{Alex}{#1}}
\newcommand{\tonianswer}[1]{\answer{Toni}{#1}}

\newcommand{\jerin}[1]{\inline{Jeremy}{#1}}
\newcommand{\toniin}[1]{\inline{Toni}{#1}}
\newcommand{\alexin}[1]{\inline{Àlex}{#1}}



\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{On the Effect of Word Order on Cross-lingual Sentiment Analysis}

\author {\textbf{{\`A}lex Ramírez Atrio$^1$}, \textbf{Toni Badia$^{1}$}, \textbf{Jeremy Barnes$^{2}$}\\[5pt]
$^1$Universitat Pompeu Fabra\\
{\tt \{toni.badia\}@upf.edu} \\[5pt]
$^2$University of Oslo\\
{\tt jeremycb@ifi.uio.no}
}

%\author{First Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  {\tt email@domain} \\\And
%  Second Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  {\tt email@domain} \\}

\date{}
\begin{document}
\maketitle
\begin{abstract}
  This document contains the instructions for preparing a camera-ready
  manuscript for the proceedings of NAACL-HLT 2019. The document itself
  conforms to its own specifications, and is therefore an example of
  what your manuscript should look like. These instructions should be
  used for both papers submitted for review and for final versions of
  accepted papers.  Authors are asked to conform to all the directions
  reported in this document.
\end{abstract}


\section{Introduction}



%When facing a relatively simple text such as an hotel review, we can ask for its general sentiment. Is it positive, or negative? Is it better to ask for more gray areas in between? Automatizing the process of classifying the sentiment of a text is called Sentiment Analysis (SA), and it can allow us to get a good understanding of how the author(s) of these texts feel about the topics that are discussed in them. Of course, there are issues that we may find in this process. For instance, given a certain text, its sentiment may be ambiguous: two independent human annotators may disagree in it. Or maybe the text does not have an overall sentiment, and we should focus on individual sentences.

Cross-lingual Sentiment Analysis (CLSA) exploits resources, \eg labeled data of a high-resource language, to train a sentiment classifier in order for it to classify low-resource languages. This is relevant when a target language lacks plentiful labeled data, particularly when considering specific domains. 

This process can be carried out using Machine Translation (MT) of the source language and training a classifier using these translated texts \cite{Banea2008,Balahur2014d}. The main problem with this approach is its high requirements of parallel annotated data, which may be difficult to find for some low-resource languages. Approaches that use bilingual distributional representations, in contrast, have been shown to be competitive while requiring less amount of parallel annotated data \cite{Chen2016,Barnes2018b}. All things equal, these approaches, therefore, are preferable.

Recently, sentiment classifiers pre-trained on a language modeling task have lead to state-of-the-art results \cite{Peters2018,Howard2018,Devlin2018}. This suggests that sentiment analysis benefits from learning word order and fine-grained relationships between tokens and allows us to make use of unlabeled data. Current approaches, however, have only been applied in a monolingual setting. 

In this work, we perform an analysis of the effect of word order on cross-lingual sentiment classifiers that use bilingual embeddings as features. We ...

\jeranswer{I was thinking specifically to include the problem of differences in word order between languages. You could explain that current state-of-the-art sentiment models are heavily influenced by word order, as they are usually pretrained language models which are
then fine-tuned for sentiment, \ie \cite{Peters2018,Howard2018,Devlin2018}}



\alex{ Should I introduce in this section the specifics? That our source language is English, our targets Spanish and Catalan and that we will be using embeddings?}
\jeranswer{Yes, but at a high level. You could also include a preview of the results we have, and what contribution that makes to the community.}


\section{Related Work}

\subsection{Cross-lingual Sentiment Analysis}

Although early approaches to cross-lingual sentiment analysis relied
heavily on machine translation \cite{Banea2008,Balahur2014d,Klinger2015},
more recently there have been

\alex{I am not sure what do you mean here. Abdallah and hirst 2017, chen et al 2016 and barnes et al acl18?}

\jeranswer{Yes, those and }

\subsection{Bilingual Word Embeddings: Artetxe and why we use these: SOTA and low-resource}

\alex{Copying the same section in the acl18 paper?}
\jeranswer{So similar information, but try to say it in your words.}

\subsection{Word Order in Sentiment}

Pre-training sentiment classifiers with a language-modeling task has
become a successful way to do transfer learning. \newcite{Peters2018} learn
to create contextualized embeddings by training a character-level convolutional
network to predict the next word in a a sequence. Similarly, \newcite{Howard2018} introduce techniques that improve the fine-tuning of the base language-model. Finally, 
\newcite{Devlin2018} introduce a self-attention network, and adjust the language
modeling task to more of a cloze task, where they predict missing words in a sentence, rather than the next word given a sequence. They then fine-tune their models on downstream tasks. All of these techniques have led to state-of-the-art results on
sentiment tasks.

\alex{Do you have any specific paper in mind? Because I don't think I found any literature in this in my thesis.}
\jeranswer{There's not really any exact literature on cross-lingual versions, but
the fact that modern sota approaches are based on language models means that they
are likely sensitive to word order. You could add information on ELMO, UMLFIT, and BERT \cite{Peters2018,Howard2018,Devlin2018}.
If you want, I can finish this part later, but you might like to have a try first.}

Because of some problems we may find in MT, sometimes it is considered best to preprocess the source language by reordering it and then carryig out the translation. Transformation (reordering) rules can be determined manually or with data-driven approaches. Their application can be deterministic or non-deterministic. Some hybrid techniques exist as well, where long-range transformations are deterministic and the rest non-deterministic.

\subsection{Cross-lingual Sentiment Analysis}

\cite{Mohammad2015b}

\subsection{Bilingual Word Embeddings}

\alex{What is the difference between this subsection an the last subsection on blse?}
\jeranswer{Not too much, but if you could use your words to describe it, it would
be very useful.}

\section{Methodology}

\subsection{Models}

In order to test our hypothesis, we compare three different classifiers: a Support Vector Machine (SVM) \todo{CITE} with Bag-of-Embeddings, a Convolutional Neural Network (CNN) \todo{CITE}, and a Bidirectional Long Short Term Memory Network (BiLSTM) \todo{CITE}. Each of these classifiers theoretically have an increasing reliance on word order. The SVM does not take into account word order at all and is therefore our order-agnostic baseline. The CNN considers only local word order, and in this respect is basically a parameterized n-gram model. Finally, the BiLSTM considers both short-term and long-term word order in both directions.

For all models, we optimize the parameters on the source-language development set and test on the target-language data.

\jer{Could you cite the relevant papers for each of these and give more details on their assumptions regarding word order? It would also be useful to explain the training process
and hyperparameter optimization.}


\subsection{Corpora and Datasets}

At document-level, bag-of-words models are often expressive enough to give good results without relying on word order. Given that we are interested in word-order effects in cross-lingual sentiment analysis, we therefore require datasets that are annotated at a fine-grained level, \ie sentence- or aspect-level.

For this reason, we use the English and Spanish OpeNER corpora of hotel reviews \cite{Agerri2013} as well as the Catalan MultiBooked Dataset \cite{Barnes2018a}. 

The corpora are annotated for Part-of-Speech tags and sentiment with 4 classes. We use the English subset for training our classifiers and the Spanish for testing different reorderings.

We also use MultiBooked, an annotated corpus of hotel reviews in Catalan. The corpus is also annotated for POS tags and we will use it to test different reorderings.

\alex{Here I am saying "reordering", but it is not only reorderings. Do you have a better word for the tests texts?}
\jeranswer{We explain the different setups and their hypotheses in the next section,
so you don't necessarily need to mention the reorderings here yet, as we're just
introducing the data.}


\alex{" motivation for using these resources": What exactly do you mean by this?}
\jeranswer{There are other corpora we could have chosen. Why are these the best
to perform controlled experiments? Ideas: 1) no domain problem, 2) same annotation scheme}


\subsection{Experimental Setup}

\begin{itemize}
\item Test all models on two cross-lingual setups (en-es, en-ca)

\alex{Maybe next item should be before this one?}
\jeranswer{I agree.}

The experiment consists on training the LSTM, CNN, and SVM with English data, and test them on different reorderings of both Spanish and Catalan corpora.

\item Compare: No reordering, Random Reordering, N-ADJ, reordering-crego, No lexicon, Only lexicon

\alex{Here we should also talk about the Machine Translation parts? Here we introduce the lexicon thing, but we have not talked about it yet, right?}
\jeranswer{Yes, let's introduce all of that here.}

We compare the original texts of the Catalan MultiBooked and Spanish OpeNER, a random reordering of these, a simple reordering consisting of the application of the the rule N-ADJ to ADJ-N, a reordering resulted of the application of 15 transformation rules extracted from Crego and Mariño 2006a and 2006b, a version of the corpora with all the words appearing in their respective lexicon deleted, and a version of the corpora with exclusively the words appearing in their respective lexicon.

\item What are the competing hypotheses for each of these setups?

\alex{Here do you mean to list different prediction for every of these setups, like: we expect n-adj to perform slightly better than the original, random to be worse, ...?}
\jeranswer{Yes I do}

\end{itemize}

\section{Results}

\alex{Falta omplir-les}


\begin{table*}[t]
\newcommand{\sep}{\cmidrule(r){4-6}\cmidrule(r){7-9}}
\newcommand{\sepp}{\cmidrule(r){4-4}\cmidrule(r){5-5}\cmidrule(r){6-6}\cmidrule(r){7-7}\cmidrule(r){8-8}\cmidrule(r){9-9}}

\definecolor{green}{RGB}{150,255,150}
\definecolor{blue}{RGB}{150,150,255}

\newcommand{\bestproj}[1]{{\setlength{\fboxsep}{0pt}\colorbox{lightblue}{\textit{#1}}}}
\newcommand{\bestoverall}[1]{{\setlength{\fboxsep}{0pt}\colorbox{lightgreen}{\textbf{#1}}}}

\setlength\tabcolsep{10pt}
\renewcommand*{\arraystretch}{0.8}
\centering\small
\begin{tabular}{lllcccccc}
\toprule
&& & \multicolumn{3}{c}{Binary} & \multicolumn{3}{c}{4-class} \\
\sep
\multirow{14}{*}{\rt{Bilingual Word Embeddings}} 
	& \multirow{6}{*}{\rt{EN-ES}}
		& Original 	&  &  &  &  &  &  \\ 
		&& Reordered  &  &  &  &  &  &  \\ 
		&& N-ADJ  &  &  &  &  &  & \\ 
		&& Random  &  &  &  &  &  & \\ 
		&& Only Lexicon  &  &  &  &  &  & \\ 
		&& No Lexicon  &  &  &  &  &  & \\ 
	\sepp
	& \multirow{6}{*}{\rt{EN-CA}}
  		& Original &  &  &  &  &  & \\ 
		&& Reordered  &  &  &  &  &  & \\ 
		&& N-ADJ &   &  &  &  &  & \\ 
		&& Random &  &  &  &  &  & \\ 
		&& Only Lexicon  &  &  &  &  &  & \\ 
		&& No Lexicon &  &  &  &  &  & \\ 


\\
\hline \\

\multirow{4}{*}{\rt{Mono}}
	& \multirow{4}{*}{\rt{EN}}
  		& Original &  &  &  &  &  & \\ 
		&& Random &  &  &  &  &  & \\ 
		&& Only Lexicon  &  &  &  &  &  & \\ 
		&& No Lexicon &  &  &  &  &  & \\ 

\\
\hline \\

\multirow{8}{*}{\rt{Machine Translation}}
	& \multirow{4}{*}{\rt{EN}}
  		& Original &  &  &  &  &  & \\ 
		&& Random &  &  &  &  &  & \\ 
		&& Only Lexicon  &  &  &  &  &  & \\ 
		&& No Lexicon &  &  &  &  &  & \\ 
	\sepp
	& \multirow{4}{*}{\rt{EN}}
  		& Original &  &  &  &  &  & \\ 
		&& Random &  &  &  &  &  & \\ 
		&& Only Lexicon  &  &  &  &  &  & \\ 
		&& No Lexicon &  &  &  &  &  & \\ 
		
\\

\bottomrule
\end{tabular}
\caption{Macro \F results for all corpora and techniques. We denote
  the best performing projection-based
  method per column with a \bestproj{blue box} and the best overall method
  per column with a
  \bestoverall{green box}.}
\label{results:all}
\end{table*}

\section{Analysis}

\jer{It would be nice to show that the noise introduced by bilingual embeddings leads to LSTMs not being able to pick up on word order in the target language. We could train monolingual models for Spanish and Catalan and use the random reordering to see the difference.}

\todo{It would also be interesting to look at particular examples of errors that each model suffers. Are they different in each model? Is there any pattern?}

\section{Conclusion and Future Work}



\bibliography{lit}
\bibliographystyle{acl_natbib}

\appendix

\section{Appendices}
\label{sec:appendix}
Appendices are material that can be read, and include lemmas, formulas, proofs, and tables that are not critical to the reading and understanding of the paper. 
Appendices should be {\bf uploaded as supplementary material} when submitting the paper for review. Upon acceptance, the appendices come after the references, as shown here. Use
\verb|\appendix| before any appendix section to switch the section
numbering over to letters.



\end{document}
