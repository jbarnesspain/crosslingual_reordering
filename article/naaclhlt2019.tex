%
% File naacl2019.tex
%
%% Based on the style files for ACL 2018 and NAACL 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{naaclhlt2019}
\usepackage{times}
\usepackage{latexsym}
\usepackage[utf8]{inputenc}

\usepackage{multirow}
\usepackage{xspace}
\usepackage{url}
\usepackage{tabularx}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{paralist}
\usepackage{subfig}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xstring}
\usepackage{scalefnt}
\usepackage{bm}
\usepackage{paralist}
%\usepackage{microtype} % microtype breaks non-ascii use in bib
\usepackage{tablefootnote}
\usepackage{subfig}
\usepackage{capt-of}
\usepackage{latexsym}
\usepackage{paralist}
\usepackage{calc}
\usepackage{booktabs}
\usepackage{url}
\usepackage{scrextend}

\newcommand\footnoteref[1]{\protected@xdef\@thefnmark{\ref{#1}}\@footnotemark}
\newcommand{\aspectbox}[1]{{\setlength{\fboxsep}{1pt}\colorbox{lightgreen}{#1}}}
\newcommand{\posbox}[1]{{\setlength{\fboxsep}{1pt}\colorbox{lightblue}{#1}}}
\newcommand{\negbox}[1]{{\setlength{\fboxsep}{1pt}\colorbox{lightred}{#1}}}

\definecolor{lightgreen}{RGB}{200,255,200}
\definecolor{lightblue}{RGB}{200,200,255}
\definecolor{lightred}{RGB}{255,200,200}

\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\mt}{\textsc{Mt}\xspace}
\newcommand{\blse}{\textsc{Blse}\xspace}
\newcommand{\barista}{\textsc{Barista}\xspace}
\newcommand{\vecmap}{\textsc{VecMap}\xspace}
\newcommand{\muse}{\textsc{Muse}\xspace}

\newcommand{\rt}[1]{\rotatebox{90}{#1}}
\newcommand{\rrt}[1]{\rotatebox{45}{#1}}

\newcommand{\ie}{\textit{i.\,e.}\xspace}
\newcommand{\eg}{\textit{e.\,g.}\xspace}
\newcommand{\F}{$\text{F}_1$\xspace}


%jer: automatically turned off when aclfinalcopy is turned on
\usepackage{annotates}
\newcommand{\todo}[1]{\comment{Todo}{#1}}
\newcommand{\jer}[1]{\comment{Jeremy}{#1}}
\newcommand{\toni}[1]{\comment{Toni}{#1}}
\newcommand{\alex}[1]{\comment{Alex}{#1}}

\newcommand{\jeranswer}[1]{\answer{Jeremy}{#1}}
\newcommand{\alexanswer}[1]{\answer{Alex}{#1}}
\newcommand{\tonianswer}[1]{\answer{Toni}{#1}}

\newcommand{\jerin}[1]{\inline{Jeremy}{#1}}
\newcommand{\toniin}[1]{\inline{Toni}{#1}}
\newcommand{\alexin}[1]{\inline{Àlex}{#1}}



\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{On the Effect of Word Order on Cross-lingual Sentiment Analysis}

\author {\textbf{Àlex R. Atrio$^1$}, \textbf{Toni Badia$^{1}$}, \textbf{Jeremy Barnes$^{2}$}\\[5pt]
$^1$Universitat Pompeu Fabra\\
{\tt \{toni.badia\}@upf.edu} \\[5pt]
{\tt \{alexratrio\}@gmail.com} \\[5pt]
$^2$University of Oslo\\
{\tt jeremycb@ifi.uio.no}
}

%\author{First Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  {\tt email@domain} \\\And
%  Second Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  {\tt email@domain} \\}

\date{}
\begin{document}
\maketitle
\begin{abstract}
  This document contains the instructions for preparing a camera-ready
  manuscript for the proceedings of NAACL-HLT 2019. The document itself
  conforms to its own specifications, and is therefore an example of
  what your manuscript should look like. These instructions should be
  used for both papers submitted for review and for final versions of
  accepted papers.  Authors are asked to conform to all the directions
  reported in this document.
\end{abstract}


\section{Introduction}



%When facing a relatively simple text such as an hotel review, we can ask for its general sentiment. Is it positive, or negative? Is it better to ask for more gray areas in between? Automatizing the process of classifying the sentiment of a text is called Sentiment Analysis (SA), and it can allow us to get a good understanding of how the author(s) of these texts feel about the topics that are discussed in them. Of course, there are issues that we may find in this process. For instance, given a certain text, its sentiment may be ambiguous: two independent human annotators may disagree in it. Or maybe the text does not have an overall sentiment, and we should focus on individual sentences.

Cross-lingual Sentiment Analysis (CLSA) exploits resources, \eg labeled data of a high-resource language, to train a sentiment classifier in order for it to classify low-resource languages. This is relevant when a target language lacks plentiful labeled data, particularly when considering specific domains. 

This process can be carried out using Machine Translation (MT) of the source language and training a classifier using these translated texts \cite{Banea2008,Balahur2014d}. The main problem with this approach is its high requirements of parallel annotated data, which may be difficult to find for some low-resource languages. Approaches that use bilingual distributional representations, in contrast, have been shown to be competitive while requiring less amount of parallel annotated data \cite{Chen2016,Barnes2018b}. All things equal, these approaches, therefore, are preferable.

Recently, sentiment classifiers pre-trained on a language modeling task have lead to state-of-the-art results \cite{Peters2018,Howard2018,Devlin2018}. This suggests that sentiment analysis benefits from learning word order and fine-grained relationships between tokens and allows us to make use of unlabeled data. Current approaches, however, have only been applied in a monolingual setting. 

In this work, we perform an analysis of the effect of word order on cross-lingual sentiment classifiers that use bilingual embeddings as features. We ...


\section{Related Work}

\subsection{Cross-lingual Sentiment Analysis}

Although most approaches to cross-lingual sentiment analysis rely on machine translation \cite{Banea2008,Balahur2014d,Klinger2015},
there are several approaches that instead rely on bilingual representations. 
\newcite{Prettenhofer2011b} adopt Structural Correspondence Learning to the
bilingual setting. They create useful bilingual features by predicting the existence
of ``pivots'', which are words that are both predictive for the task in both source and target languages.

Bilingual word embeddings \cite{Kocisky2014,Chandar2014,Luong2015} now provide
a simple framework for cross-lingual approaches to natural language processing. 
Current state-of-the-art approaches \cite{Artetxe2017,Artetxe2018,Lample2017} first separately create monolingual embeddings using large, unlabeled corpora in the source and target languages. They then learn an orthogonal projection from the source to the target space in a supervised manner using a bilingual lexicon. The orthogonality constrain maintains the relations that exist between the words before projection.

Bilingual word embeddings are now used as features for state-of-the-art document-level \cite{Chen2016}, sentence-level \cite{Barnes2018b}, and targeted \cite{Hangya2018} cross-lingual sentiment analysis approaches.


\subsection{Word Order in Sentiment}

Pre-training sentiment classifiers with a language-modeling task has
become a successful way to do transfer learning. \newcite{Peters2018} learn
to create contextualized embeddings by training a character-level convolutional
network to predict the next word in a a sequence. Similarly, \newcite{Howard2018} introduce techniques that improve the fine-tuning of the base language-model. Finally, 
\newcite{Devlin2018} introduce a self-attention network, and adjust the language
modeling task to more of a cloze task, where they predict missing words in a sentence, rather than the next word given a sequence. They then fine-tune their models on downstream tasks. All of these techniques have led to state-of-the-art results on
sentiment tasks.

\jer{Alex, can you add the relevant citations here?}
Because of some problems we may find in MT, sometimes it is considered best to preprocess the source language by reordering it and then carrying out the translation. Transformation (reordering) rules can be determined manually or with data-driven approaches. Their application can be deterministic or non-deterministic. Some hybrid techniques exist as well, where long-range transformations are deterministic and the rest non-deterministic.

\subsection{Cross-lingual Sentiment Analysis}

\cite{Mohammad2015b}

\subsection{Bilingual Word Embeddings}

\alex{What is the difference between this subsection an the last subsection on blse?}
\jeranswer{Not too much, but if you could use your words to describe it, it would
be very useful.}

\section{Methodology}

\subsection{Corpora and Datasets}

\begin{table}[tb]
\centering%\small
\begin{tabular}{lrrrr}
\toprule
    & & \multicolumn{1}{c}{EN} & \multicolumn{1}{c}{ES} & \multicolumn{1}{c}{CA} \\
\cmidrule(rl){2-2}\cmidrule(l){3-3}\cmidrule(l){4-4}\cmidrule(l){5-5}
 \multirow{3}{*}{\rt{Binary}}
 &$+$   & 1258 & 1216 & 718     \\
 &$-$   & 473 & 256 & 467   \\
 &\textit{Total}    &1731 & 1472  &   1185         \\
\cmidrule(rl){2-2}\cmidrule(l){3-3}\cmidrule(l){4-4}\cmidrule(l){5-5}
 \multirow{5}{*}{\rt{4-class}}
 &$++$   & 379 & 370  & 256  \\
 &$+$    & 879 & 846  & 462   \\
 &$-$    & 399 & 218  & 409    \\
 &$--$   &  74 & 38   & 58     \\
 &\textit{Total}     & 1731  & 1472     & 1185       \\
\bottomrule
\end{tabular}
\caption{Statistics for the OpeNER English (EN) and Spanish (ES) 
as well as the MultiBooked Catalan (CA) datasets.}
\label{datasetstats}
\end{table}

At document-level, bag-of-words models are often expressive enough to give good results without relying on word order. Given that we are interested in word-order effects in cross-lingual sentiment analysis, we therefore require datasets that are annotated at a fine-grained level, \ie sentence- or aspect-level.

For this reason, we use the English and Spanish OpeNER corpora of hotel reviews \cite{Agerri2013} as well as the Catalan MultiBooked Dataset \cite{Barnes2018a}. 

The corpora are annotated for Part-of-Speech tags and sentiment with 4 classes. We use the English subset for training our classifiers and the Spanish and Catalan for testing the effects of word order on the target languages.

Although these datasets are relatively small, they are all annotated similarly and
are in domain, which avoids problems with mapping labels or domain shifts.

\subsection{Experimental Setup}

\begin{table*}[]
\centering\small
\begin{tabular}{ll}
\toprule
\emph{Original} & Unico punto \negbox{negativo} el \negbox{ruido} que las ventanas de madera tan típicas de la zona \negbox{no consiguen} aislar totalmente. \\[3pt]
\emph{Noun-adjective} & Unico \negbox{negativo} punto el \negbox{ruido} que las ventanas de madera tan típicas de la zona \negbox{no consiguen} aislar totalmente \\[3pt]
\emph{MT-inspired} & Unico \negbox{negativo} punto el \negbox{ruido} que las ventanas de tan típicas madera de la \negbox{no} zona \negbox{consiguen} aislar totalmente \\[3pt]
\emph{Only-lexicon} & \negbox{negativo} \negbox{ruido} aislar \\[3pt]
\emph{No-lexicon} & Unico punto el que las ventanas de madera tan típicas de la zona \negbox{no consiguen} totalmente \\[3pt]
\emph{Random} & aislar madera consiguen típicas de el de totalmente zona las ventanas punto \negbox{negativo} Unico la \negbox{no} \negbox{ruido} tan que\\[3pt]
\bottomrule
\end{tabular}
\caption{An example of a negative sentence (original) with the five reordering
transformations applied.}
\label{example}

\end{table*}

In order to test whether a sentiment classifier trained on bilingual embeddings is sensitive to word order, we test our trained classifiers on six versions of the test
data, which we describe in the following. An example of these six versions is shown in Table \ref{example}. TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT 

\paragraph{No Reordering: }The model is tested on the original data.

\paragraph{Random Reordering: }We randomly permutate the order of the target language sentences. If the sentiment classification models take the target language word order into consideration, this should lead to poor results.

\paragraph{Noun-adjective Switch: }Given that adjectives are important for sentiment analysis, we hypothesize that adjusting the order of nouns and adjectives should be beneficial if the classifier is learning source-language word order. Therefore, we implement a simple reordering where Spanish and Catalan adjectives are placed in front of the noun they modify, rather than following it.

\paragraph{MT-style pre-reordering: }A competing hypothesis is that a full pre-reordering of the target-side sequences will be more familiar to the sentiment classifier trained on English and therefore lead to better results. We implement a version of linguistically motivated rewrite rules \cite{Crego2006,Crego2006b}.

\paragraph{Only-lexicon and No-lexicon: }Finally, we provide two baselines for clarification. The \emph{Only-Lexicon} experiment removes all words which do
not appear in the Hu \& Liu sentiment lexicon \cite{HuandLiu2004}. If our systems
take word order into account, they should be affected negatively by this, as the
resulting sentence does not resemble the normal word order. If, however, the models
are simply relying on keywords, this will have little effect.

For the \emph{no-lexicon} experiment, we remove all of the words
in a phrase which are found in the sentiment lexicon. This should lead to the worst performance. 

\subsection{Models}

In order to test our hypothesis, we compare three different classifiers: a Support Vector Machine (SVM) \todo{CITE} with Bag-of-Embeddings, a Convolutional Neural Network (CNN) \todo{CITE}, and a Bidirectional Long Short Term Memory Network (BiLSTM) \todo{CITE}. Each of these classifiers theoretically have an increasing reliance on word order. The SVM does not take into account word order at all and is therefore our order-agnostic baseline. The CNN considers only local word order, and in this respect is basically a parameterized n-gram model. Finally, the BiLSTM considers both short-term and long-term word order in both directions.

For all models, we optimize the parameters on the source-language development set and test on the target-language data.

\jer{Could you cite the relevant papers for each of these and give more details on their assumptions regarding word order? It would also be useful to explain the training process
and hyperparameter optimization.}



\section{Results}

\alex{Falta omplir-les}


\begin{table*}[]
\newcommand{\sep}{\cmidrule(r){4-6}\cmidrule(r){7-9}}
\newcommand{\sepp}{\cmidrule(r){4-4}\cmidrule(r){5-5}\cmidrule(r){6-6}\cmidrule(r){7-7}\cmidrule(r){8-8}\cmidrule(r){9-9}}

\definecolor{green}{RGB}{150,255,150}
\definecolor{blue}{RGB}{150,150,255}

\newcommand{\bestproj}[1]{{\setlength{\fboxsep}{0pt}\colorbox{lightblue}{\textit{#1}}}}
\newcommand{\bestoverall}[1]{{\setlength{\fboxsep}{0pt}\colorbox{lightgreen}{\textbf{#1}}}}

\setlength\tabcolsep{10pt}
\renewcommand*{\arraystretch}{0.8}
\centering\small
\begin{tabular}{lllcccccccccccc}
\toprule
&& & \multicolumn{3}{c}{Binary} & \multicolumn{3}{c}{4-class} \\
\sep
\multirow{14}{*}{\rt{Bilingual Word Embeddings}} 
	& \multirow{6}{*}{\rt{EN-ES}}
	    && BiLSTM & CNN & SVM & BiLSTM & CNN & SVM \\
	    \cmidrule(r){4-4}\cmidrule(r){5-5}\cmidrule(r){6-6}\cmidrule(r){7-7}\cmidrule(r){8-8}\cmidrule(r){9-9}
		&& Original 	 &  & 0.616 &  &  & 0.359 &  \\ 
		&& Reordered  &  & 0.619 &  &  & 0.357 &  \\ 
		&& N-ADJ  &  & 0.616 &  &  & 0.363 & \\ 
		&& Random  &  & 0.604 &  &  & 0.342 & \\ 
		&& Only Lexicon  &  & ? &  &  & ? & \\ 
		&& No Lexicon  &  & ? &  &  & ? & \\ 
	\sepp
	& \multirow{6}{*}{\rt{EN-CA}}
  		& Original &  & 0.682 &  &  & 0.319 & \\ 
		&& Reordered  &  & 0.684 &  &  & 0.323 & \\ 
		&& N-ADJ &   & 0.685 &  &  & 0.318 & \\ 
		&& Random &  & 0.669 &  &  & 0.317 & \\ 
		&& Only Lexicon  &  & ? &  &  & ? & \\ 
		&& No Lexicon &  & ? &  &  & ? & \\ 


\\
\hline \\

\multirow{4}{*}{\rt{Mono}}
	& \multirow{4}{*}{\rt{EN}}
  		& Original &  & 0.945 &  &  & 0.856 & \\ 
		&& Random &  & 0.847 &  &  & 0.641 & \\ 
		&& Only Lexicon  &  & ? &  &  & ? & \\ 
		&& No Lexicon &  & ? &  &  & ? & \\ 

\\
\hline \\

\multirow{8}{*}{\rt{Machine Translation}}
	& \multirow{4}{*}{\rt{EN (ES)}}
  		& Original &  & 0.713 &  &  & 0.481 & \\ 
		&& Random &  & 0.710 &  &  & 0.480 & \\ 
		&& Only Lexicon  &  & ? &  &  & ? & \\ 
		&& No Lexicon &  & ? &  &  & ? & \\ 
	\sepp
	& \multirow{4}{*}{\rt{EN (CA)}}
  		& Original &  & 0.752 &  &  & 0.487 & \\ 
		&& Random &  & 0.761 &  &  & 0.463 & \\ 
		&& Only Lexicon  &  & ? &  &  & ? & \\ 
		&& No Lexicon &  & ? &  &  & ? & \\ 
		
\\

\bottomrule
\end{tabular}
\caption{Macro \F results for all corpora and techniques. We denote
  the best performing projection-based
  method per column with a \bestproj{blue box} and the best overall method
  per column with a
  \bestoverall{green box}.}
\label{results:all}
\end{table*}

\section{Analysis}

\jer{It would be nice to show that the noise introduced by bilingual embeddings leads to LSTMs not being able to pick up on word order in the target language. We could train monolingual models for Spanish and Catalan and use the random reordering to see the difference.}

\todo{It would also be interesting to look at particular examples of errors that each model suffers. Are they different in each model? Is there any pattern?}

\section{Conclusion and Future Work}



\bibliography{lit}
\bibliographystyle{acl_natbib}

\appendix

\section{Appendices}
\label{sec:appendix}
Appendices are material that can be read, and include lemmas, formulas, proofs, and tables that are not critical to the reading and understanding of the paper. 
Appendices should be {\bf uploaded as supplementary material} when submitting the paper for review. Upon acceptance, the appendices come after the references, as shown here. Use
\verb|\appendix| before any appendix section to switch the section
numbering over to letters.



\end{document}
